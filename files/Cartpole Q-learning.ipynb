{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Q-learning to solve CartPole problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries\n",
    "- gym contains the environment\n",
    "- math is for some math functions\n",
    "- numpy is for managing data\n",
    "- pyplot and seaborn are for data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create agent\n",
    "Most of this code was originally implemented by Isaac Patole. His work can be accessed [here](https://github.com/IsaacPatole/CartPole-v0-using-Q-learning-SARSA-and-DNN/blob/master/Qlearning_for_cartpole.py). However, some comments were added to the code and additional functions were implemented so that the ensemble model could access a probability vector for decision making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Base code taken from: \n",
    "https://github.com/IsaacPatole/CartPole-v0-using-Q-learning-SARSA-and-DNN/blob/master/Qlearning_for_cartpole.py\n",
    "\"\"\"\n",
    "\n",
    "class CartPoleQAgent():\n",
    "    def __init__(self, buckets=(3, 3, 6, 6), \n",
    "                 num_episodes=500, min_lr=0.1, \n",
    "                 min_epsilon=0.1, discount=1.0, decay=25):\n",
    "        self.buckets = buckets\n",
    "        self.num_episodes = num_episodes\n",
    "        self.min_lr = min_lr\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.discount = discount\n",
    "        self.decay = decay\n",
    "\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        \n",
    "        # This is the action-value function being initialized to 0's\n",
    "        self.Q_table = np.zeros(self.buckets + (self.env.action_space.n,))\n",
    "\n",
    "        # [position, velocity, angle, angular velocity]\n",
    "        self.upper_bounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], math.radians(50) / 1.]\n",
    "        self.lower_bounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], -math.radians(50) / 1.]\n",
    "        \n",
    "        #\n",
    "        self.steps = np.zeros(self.num_episodes)\n",
    "        \n",
    "        \n",
    "\n",
    "    def discretize_state(self, obs):\n",
    "        \"\"\"\n",
    "        Takes an observation of the environment and aliases it.\n",
    "        By doing this, very similar observations can be treated\n",
    "        as the same and it reduces the state space so that the \n",
    "        Q-table can be smaller and more easily filled.\n",
    "        \n",
    "        Input:\n",
    "        obs (tuple): Tuple containing 4 floats describing the current\n",
    "                     state of the environment.\n",
    "        \n",
    "        Output:\n",
    "        discretized (tuple): Tuple containing 4 non-negative integers smaller \n",
    "                             than n where n is the number in the same position\n",
    "                             in the buckets list.\n",
    "        \"\"\"\n",
    "        discretized = list()\n",
    "        for i in range(len(obs)):\n",
    "            scaling = ((obs[i] + abs(self.lower_bounds[i])) \n",
    "                       / (self.upper_bounds[i] - self.lower_bounds[i]))\n",
    "            new_obs = int(round((self.buckets[i] - 1) * scaling))\n",
    "            new_obs = min(self.buckets[i] - 1, max(0, new_obs))\n",
    "            discretized.append(new_obs)\n",
    "        return tuple(discretized)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Implementation of e-greedy algorithm. Returns an action (0 or 1).\n",
    "        \n",
    "        Input:\n",
    "        state (tuple): Tuple containing 4 non-negative integers within\n",
    "                       the range of the buckets.\n",
    "        \n",
    "        Output:\n",
    "        (int) Returns either 0 or 1\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample() \n",
    "        else:\n",
    "            return np.argmax(self.Q_table[state])\n",
    "        \n",
    "    def get_action(self, state, e):\n",
    "        \"\"\"\n",
    "        Another policy based on the Q-table. Slight variation from \n",
    "        e-greedy. It assumes the state fed hasn't been discretized and \n",
    "        returns a vector with probabilities for each action.\n",
    "        \n",
    "        Input: \n",
    "        state (tuple): Contains the 4 floats used to describe\n",
    "                       the current state of the environment.\n",
    "        e (int): Denotes the episode at which the agent is supposed\n",
    "                 to be, helping balance exploration and exploitation.\n",
    "                 \n",
    "        Output:\n",
    "        action_vector (numpy array): Vector containing the probability\n",
    "                                     of each action being chosen at the\n",
    "                                     current state.\n",
    "        \"\"\"\n",
    "        obs = self.discretize_state(state)\n",
    "        action_vector = self.Q_table[obs]\n",
    "        epsilon = self.get_epsilon(e)\n",
    "        action_vector = self.normalize(action_vector, epsilon)\n",
    "        return action_vector\n",
    "\n",
    "    def normalize(self, action_vector, epsilon):\n",
    "        \"\"\"\n",
    "        Returns a vector with components adding to 1. Ensures \n",
    "        \n",
    "        Input:\n",
    "        action_vector (numpy array): Contains expected values for each\n",
    "                                     action at current state from Q-table.\n",
    "        epsilon (float): Chances that the e-greedy algorithm would \n",
    "                         choose an action at random. With this pol\n",
    "        \n",
    "        Output:\n",
    "        new_vector (numpy array): Vector containing the probability\n",
    "                                  of each action being chosen at the\n",
    "                                  current state.\n",
    "        \"\"\"\n",
    "        \n",
    "        total = sum(action_vector)\n",
    "        new_vector = (1-epsilon)*action_vector/(total)\n",
    "        new_vector += epsilon/2.0\n",
    "        return new_vector\n",
    "\n",
    "    def update_q(self, state, action, reward, new_state):\n",
    "        \"\"\"\n",
    "        Updates Q-table using the rule as described by Sutton and Barto in\n",
    "        Reinforcement Learning.\n",
    "        \"\"\"\n",
    "        self.Q_table[state][action] += (self.learning_rate * \n",
    "                                        (reward \n",
    "                                         + self.discount * np.max(self.Q_table[new_state]) \n",
    "                                         - self.Q_table[state][action]))\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        \"\"\"Gets value for epsilon. It declines as we advance in episodes.\"\"\"\n",
    "        # Ensures that there's almost at least a min_epsilon chance of randomly exploring\n",
    "        return max(self.min_epsilon, min(1., 1. - math.log10((t + 1) / self.decay)))\n",
    "\n",
    "    def get_learning_rate(self, t):\n",
    "        \"\"\"Gets value for learning rate. It declines as we advance in episodes.\"\"\"\n",
    "        # Learning rate also declines as we add more episodes\n",
    "        return max(self.min_lr, min(1., 1. - math.log10((t + 1) / self.decay)))\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains agent making it go through the environment and choose actions\n",
    "        through an e-greedy policy and updating values for its Q-table. The \n",
    "        agent is trained by default for 500 episodes with a declining \n",
    "        learning rate and epsilon values that with the default values,\n",
    "        reach the minimum after 198 episodes.\n",
    "        \"\"\"\n",
    "        # Looping for each episode\n",
    "        for e in range(self.num_episodes):\n",
    "            # Initializes the state\n",
    "            current_state = self.discretize_state(self.env.reset())\n",
    "\n",
    "            self.learning_rate = self.get_learning_rate(e)\n",
    "            self.epsilon = self.get_epsilon(e)\n",
    "            done = False\n",
    "            \n",
    "            # Looping for each step\n",
    "            while not done:\n",
    "                self.steps[e] += 1\n",
    "                # Choose A from S\n",
    "                action = self.choose_action(current_state)\n",
    "                # Take action\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                new_state = self.discretize_state(obs)\n",
    "                # Update Q(S,A)\n",
    "                self.update_q(current_state, action, reward, new_state)\n",
    "                current_state = new_state\n",
    "                \n",
    "                # We break out of the loop when done is False which is\n",
    "                # a terminal state.\n",
    "        print('Finished training!')\n",
    "    \n",
    "    def plot_learning(self):\n",
    "        \"\"\"\n",
    "        Plots the number of steps at each episode and prints the\n",
    "        amount of times that an episode was successfully completed.\n",
    "        \"\"\"\n",
    "        sns.lineplot(range(len(self.steps)),self.steps)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Steps\")\n",
    "        plt.show()\n",
    "        t = 0\n",
    "        for i in range(self.num_episodes):\n",
    "            if self.steps[i] == 200:\n",
    "                t+=1\n",
    "        print(t, \"episodes were successfully completed.\")\n",
    "        \n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Runs an episode while displaying the cartpole environment.\"\"\"\n",
    "        self.env = gym.wrappers.Monitor(self.env,'cartpole')\n",
    "        t = 0\n",
    "        done = False\n",
    "        current_state = self.discretize_state(self.env.reset())\n",
    "        while not done:\n",
    "                self.env.render()\n",
    "                t = t+1\n",
    "                action = self.choose_action(current_state)\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                new_state = self.discretize_state(obs)\n",
    "                current_state = new_state\n",
    "            \n",
    "        return t   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent and plot results\n",
    "The following function returns an agent trained with the default values. In its process, it also displays its progress during the training session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def load_q_learning():\n",
    "    agent = CartPoleQAgent()\n",
    "    agent.train()\n",
    "    agent.plot_learning()\n",
    "\n",
    "    return agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type numpy.ndarray doesn't define __round__ method",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m agent \u001B[38;5;241m=\u001B[39m \u001B[43mload_q_learning\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[11], line 3\u001B[0m, in \u001B[0;36mload_q_learning\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_q_learning\u001B[39m():\n\u001B[1;32m      2\u001B[0m     agent \u001B[38;5;241m=\u001B[39m CartPoleQAgent()\n\u001B[0;32m----> 3\u001B[0m     \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m     agent\u001B[38;5;241m.\u001B[39mplot_learning()\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m agent\n",
      "Cell \u001B[0;32mIn[10], line 147\u001B[0m, in \u001B[0;36mCartPoleQAgent.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;66;03m# Looping for each episode\u001B[39;00m\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_episodes):\n\u001B[1;32m    146\u001B[0m     \u001B[38;5;66;03m# Initializes the state\u001B[39;00m\n\u001B[0;32m--> 147\u001B[0m     current_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdiscretize_state\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearning_rate \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_learning_rate(e)\n\u001B[1;32m    150\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepsilon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_epsilon(e)\n",
      "Cell \u001B[0;32mIn[10], line 51\u001B[0m, in \u001B[0;36mCartPoleQAgent.discretize_state\u001B[0;34m(self, obs)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(obs)):\n\u001B[1;32m     49\u001B[0m     scaling \u001B[38;5;241m=\u001B[39m ((obs[i] \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mabs\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlower_bounds[i])) \n\u001B[1;32m     50\u001B[0m                \u001B[38;5;241m/\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupper_bounds[i] \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlower_bounds[i]))\n\u001B[0;32m---> 51\u001B[0m     new_obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28;43mround\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuckets\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mscaling\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     52\u001B[0m     new_obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuckets[i] \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;241m0\u001B[39m, new_obs))\n\u001B[1;32m     53\u001B[0m     discretized\u001B[38;5;241m.\u001B[39mappend(new_obs)\n",
      "\u001B[0;31mTypeError\u001B[0m: type numpy.ndarray doesn't define __round__ method"
     ]
    }
   ],
   "source": [
    "agent = load_q_learning()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
